{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Older Self Say Wha?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stolen Shamelessly From [I Am Trask](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "### I HIGHLY recommend his blog for basic intros to Neural Network hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN's are for adding the dimension of sequence to the network, just as CNN's added spatial dimensionality.\n",
    "This is why they are so powerful for NLP, specifically text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's start small and teach an algorithm to add.  The \"sequence\" here is the sum of each bit of the binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[ 3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[ 3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[ 3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[ 3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[ 3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[ 2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[ 0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[ 1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[ 0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[ 0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Literal cut and paste from Andrew Trask's blog above \"\"\"\n",
    "\n",
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print \"Error:\" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print str(a_int) + \" + \" + str(b_int) + \" = \" + str(out)\n",
    "        print \"------------\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"Unrolling\" is the term for how far to extend the network in time before computing backprop.  8 in the case of the binary digits above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "### Let's defer to [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "Text (of Moby Dick) is from [Project Guttenberg](https://www.gutenberg.org/) with the license information stripped out (sorry!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1215279 characters, 81 unique.\n",
      "----\n",
      " k_f.Ah-&Q&[kJMegLuH?J3cs$_AGao4;!YMifepQnuQW;N qrxqv(E4RrhS-$kP,_M_f-Qy)bsedJNkR5$QS0r VnoA&_&,jBZm10kJ[Q1bIBXfYR[H&u\"Xd YFX0pi0[Cu$Lyq-\";q:cKnG)X\"5?KWzZ-ny.l[(??H39uj&CR&aL1[39.OI_Aa?32SMBXyt!\"\n",
      "Nk2G) \n",
      "----\n",
      "iter 0, loss: 109.861221\n",
      "----\n",
      " ;tauyde etLdtaaenooeivnVdOdlropIElnth;Mms lwaa  h,iqd hborrdqoo lshoitgSSpaDodLt.o Sne.Ah hcrredttvqsd:,Zl dh\n",
      "oenoafiso \n",
      "L usSeahetdewwma  v ru l ieos w]rAthwte sgiatide\n",
      "iveshoarseo,eamd  thod;clwln y \n",
      "----\n",
      "iter 100, loss: 109.462164\n",
      "----\n",
      " h\n",
      "uS infau cn .s ii-r ,niHp leIurE egnaf\n",
      "Nnlfe Men\n",
      "nSisott sifdewefhrE eieoAaasorR a wol   hlretTfo! huc w tnott1enAog\n",
      ".sDUr\n",
      "i eUbE ohIr  wdw.kip.Qr ahr-H\n",
      "toxmrsheHCloaonlr iwi oh ofn\n",
      " isnn. w he  hnm \n",
      "----\n",
      "iter 200, loss: 107.504696\n",
      "----\n",
      " fwiinA sGTetS vFa  La en\n",
      " fhmoA udwRrBhRo \n",
      " \"RldI -Qo'.hdmAiCrSh\n",
      "or eieo\n",
      "oaP\"Of\n",
      "BiermLEw t LR-lC Ihwen 0eSe mnws 9 isevvl\n",
      "uaeiDrti-ln WsWci Pa r tiEedd wgiunfts.a\n",
      ",BFt  hfIMy\n",
      "\n",
      "w\" o taab' \"Pw.Ee saiint \n",
      "----\n",
      "iter 300, loss: 105.799227\n",
      "----\n",
      " eet r ttt dse n  Or huA ths ieoasots  bou\n",
      "ctow  enn s  eo wee gowonenba\n",
      " a  w  oit,n rbCs,ode w u H  r mi.rpn tpnltlSoa d F\n",
      ".T oniOEireii ss  smhu\n",
      " eni  Ap Vtheeet t.a\n",
      " an fGi ule u n\n",
      "wh\"reaelua  wah  \n",
      "----\n",
      "iter 400, loss: 103.995916\n",
      "----\n",
      " ieriaoe c \n",
      " VP gne d t o   hE\n",
      "dhahtheeoiroeo ltgee rfe hed atr vnov,  menleas s  uTao\n",
      "adg oeira\n",
      "rannnois eoar- \n",
      "vaey   oomc iscegiies ah ot  \n",
      "n oLllrhaemf HMd, .Loleraaen  s\"d btht\n",
      "L l e LOA Rpf, pr   \n",
      "----\n",
      "iter 500, loss: 101.874397\n",
      "----\n",
      " oerri  oi\n",
      "sinie  hUNbl\n",
      " Hvnt(lprr-iae b,e b\n",
      "rl s hn lecYeie  tISCuatteago irefe othaoa'dig W F-x bq\n",
      "R5 os\n",
      "e dsithl ln\"f\n",
      "oTeawgsrntr hnde heomd,  asew,.sc taeare eeta !lWth eo arwhe aer\"e-ir f T rL,oiu \n",
      "----\n",
      "iter 600, loss: 99.758238\n",
      "----\n",
      " on reeApaitee   w.  L FBheBeuunoscue   heinwhntrb : i!\" The \n",
      "al\"ides  ,,  Tasdalsm ratfol n\"\"hns toe !  phet  ahed  Cire  NaaSsi\"sw'  t\n",
      "hmod tend  wdr  E TRu tom ; lpulsYThe H oid  aa the \"e\n",
      "banisteym \n",
      "----\n",
      "iter 700, loss: 97.658940\n",
      "----\n",
      " pimsYsbnp gauwic-sir h-tet.lranrft-ralRthe avLltson\" oIso - Wa\"heg-GB N; [ .FI  LEth war eefo r  sparee   -b\n",
      " AEEUFG YLTE.\n",
      "MIF TEB'e  pEAS4T\n",
      " AWISEAXh,.n ESOU ULNW. n\" N.sAYhe  dieE rhodtheole yit\n",
      "lic \n",
      "----\n",
      "iter 800, loss: 95.751208\n",
      "----\n",
      " rnng linss.w md ;heprc infd. anan op mnliletc uti\" hN, feurenvy, teonurenmendnin mIharyIhhans ae yo tarelbo glel ande n  dr, sontiv.sin bein he ae  W py cionaue  ong nun\n",
      " . thnm hyirsuultigd  in mo sf \n",
      "----\n",
      "iter 900, loss: 93.532874\n",
      "----\n",
      " eoattheu\" he s Atnnce gasw\n",
      "nd oymthe s delen llath thexb\n",
      "toncing Temundal ln, lea whe,, aoe\n",
      "anpher whekisthire oacAsonteam Dt sielp the ghy  thil theis w. sisys t authad \n",
      "s anss the thndliet\n",
      "iseeti th \n",
      "----\n",
      "iter 1000, loss: 91.208038\n",
      "----\n",
      " uitore \n",
      "y  if sin the bos the s, bt s ros ,oulgogitew .Ooopeiy Cf\"\n",
      "opSat t n so. eut Sriy sremeyu areud  Rol usinl uitOot me hispiceuregheg t  horwit thitian waan aln Wo imt b ace. whind po methe dhas \n",
      "----\n",
      "iter 1100, loss: 89.002949\n",
      "----\n",
      "  Le?renp whiipo, ors vr sem yheeadlesdP-wtace whe thanghety toteedele rovin amsd -o spa;d ilepimu kerofe amtteac,  -un .osecndr her eme hansts -oaptamteethaney wan, aly Ghtimy ein, syaleucmerp \n",
      "ecaasp \n",
      "----\n",
      "iter 1200, loss: 87.003669\n",
      "----\n",
      " h, sectaon an mor \n",
      "aont Fay, scthe \" ang pinics snwhlpg anve  alt gh,bno og Imerig Is.NAmheleoces phy th\n",
      "the \n",
      "wibse ales s ioet ioth palp sogey vaf the  af\n",
      "Aavooft meig ang thates thitoe  aogs shmegya \n",
      "----\n",
      "iter 1300, loss: 85.061841\n",
      "----\n",
      " ram\n",
      "kelats -mallaleath\n",
      " hi deNhe me ofd eojaco anger thasp the sho wo t of.oretari-waoas hadeling aet moo, Puin pew tAN hocif woang ald\n",
      " at\n",
      "lot mu oky Ifanhethey ho be  whelIIon, do cegoy ododmapos ol \n",
      "----\n",
      "iter 1400, loss: 83.200828\n",
      "----\n",
      "  deralrotet th bl iberdt\n",
      "and en freodhtan cang, f. septouser nt cy cere dileson\n",
      " thoullicheneties, nt. bales or,orinsbunaed\n",
      " aoIcrermf ongite ane coon btsos wocoI thil indserebbend\n",
      "tonf the yhenwro ea \n",
      "----\n",
      "iter 1500, loss: 81.513397\n",
      "----\n",
      " f dnod CBas?st-hanty bad -whcar danvasditisMely themos d nil PFpasyRnk doed ond ied, mageust toole liy \n",
      "he bind antere ssint ond pod thaddin Esermblry thandi, nofn nols wamturaippongep anderktwoas oug \n",
      "----\n",
      "iter 1600, loss: 80.018562\n",
      "----\n",
      " por tiingen\n",
      " s fro nref\n",
      "the theanejll seridaskbesi and ncutadetleulby nos\n",
      "wof ay Deemalt Wasungr theruoss ceaecaydtiettildn wiedd Laadhang end is acd, \n",
      "hy range nadd bin? viase luan thigs,  to this, I \n",
      "----\n",
      "iter 1700, loss: 78.579441\n",
      "----\n",
      " y ot anl, be on boms ,hult ot -s. whore bs aitharasorsd che\n",
      "causang azte-B cie sayd ons\n",
      "outenpans,\n",
      "f-Hrl mou ontursepion thic\n",
      "lta.s zeseelt fhie fald. waed eushd crajc ndo whrermeme uund,\n",
      "arpt whaado  \n",
      "----\n",
      "iter 1800, loss: 77.243354\n",
      "----\n",
      " Sart Itiand - bidy Whe,.-Thed. bb\n",
      "coeghert eiss aneT mond bo done theer the mareng gomeble. eovle\n",
      "d\n",
      "fachic\n",
      "the ft thelxI Afis ethrigd dotu iy \"hocoKkon-eot the se-oay on arit, \n",
      "ytilepont aingerShet th \n",
      "----\n",
      "iter 1900, loss: 76.117534\n",
      "----\n",
      " overeorworrasg?eput, mod eld th rn heyG- iant an rve poilcl couyo the bet, roxounlo anrermeicht cnssed to thelkr, Ior, heor. da'ore ,trer;agd de clhinn ont the tad -epans d ondr, sher soeg if.\n",
      "HE Ne v \n",
      "----\n",
      "iter 2000, loss: 75.114740\n",
      "----\n",
      " veinsshonse awr mo tend\n",
      "Toas than hatitege !orsernt of therlien as sbiI in s con if an, hi like sinn\n",
      " u? th athe, of cheot ingtow sh. cf foomet-eiceetmed Io oner, vins orbot satssod munsvinhen sathons \n",
      "----\n",
      "iter 2100, loss: 73.916046\n",
      "----\n",
      " eroidide bavedlesriv, ont rime hn orhed ancn thoge, ant thokithaint' thir inos\n",
      "rof, thinoEat. inh the t ws eilt\n",
      "s the- irtsino. the thin iroug gonn ant I thangegt sl thinly hins oul\" hit h im thea gon \n",
      "----\n",
      "iter 2200, loss: 72.804807\n",
      "----\n",
      " arad, thandid hat av  fanc beog mhaha thilevky s serly R aut Lamend anghan dog fie, ameche alek -eegh ase waas? an mos und Iele; aity boub virlpans ont.\n",
      "\n",
      "\n",
      "ur oet ind lionKtor en't I1tC-o pohery osd ap \n",
      "----\n",
      "iter 2300, loss: 71.734007\n",
      "----\n",
      " an,  oinn Itos\n",
      "oi the cand rand t and -he-lr cofwof lind D bung amut mansout, mise\" coatd tond ttaml u theswarn jtondh, ord\n",
      "yrung sorpe, gly hof hcan, t, nbohy unant, be a glondas, one com sof,\n",
      "te\n",
      "too \n",
      "----\n",
      "iter 2400, loss: 70.912535\n",
      "----\n",
      " rhe. .rout ogg slingenlat ow tharrn thee care serern win. dend ot thayhererrlores akt.wlospln of bet hermong,ro? laumen-a'le\n",
      " to chrfs, yAytrand\n",
      "hlort art, brumat yle hed ld katalptha  toA alk whaonls \n",
      "----\n",
      "iter 2500, loss: 69.999552\n",
      "----\n",
      " ee flond inke lot in by I lt tf an Wande j arhe af t I to So'-cuad ake outud ge the\n",
      "in\n",
      "cot, yhey be date ave\n",
      "l the\n",
      "taon on om hiagome Soomle ro, hintan ior that meled or bome ookle sabe; hardy, mofthe \n",
      "----\n",
      "iter 2600, loss: 68.877969\n",
      "----\n",
      " ho thoo-\n",
      "Doveigedver atorhid inneinhistinf-ke sn hinind lav non le leoher. Ie\" ptot'of thasin; h tus\n",
      "an urrined ors woom.-leling ot mom hiiged-the alt,\n",
      ", pus alig led moordprs, coorle ralagit IrDoct m \n",
      "----\n",
      "iter 2700, loss: 68.063914\n",
      "----\n",
      "  andhls eas int itale afle tled;inm, thud ont lipoonden\n",
      "tibet\n",
      "andorord she cokincmide t soud fel's thareriwfelpepurp fe, the wowet oror.. aveveis\n",
      " cohor hith comit oblele hitourewk oudiC fimy waap, ar \n",
      "----\n",
      "iter 2800, loss: 67.384688\n",
      "----\n",
      " n habinl seore.\n",
      "\n",
      "qot uny be Iere, ta, Whiicu thin t It was routat cateid is coom sastt wof whe\n",
      "oubllre und domdon hewe al, abollr, ontainecs\"I jutm pinh nat ounghe\n",
      "d cougecit, baced has t'polnt\n",
      "drlt m \n",
      "----\n",
      "iter 2900, loss: 66.799889\n",
      "----\n",
      " ear os tRthoh dirpry nufn ae wt borr sumr sawloo ' iczt-pominet oh ay O.\n",
      "\"ed olt th the lrise tou\n",
      "oud ghounky wiblangthln hoot qhat seuthinperlrersy fto-thed on arp and his be use moungof waornniig wo \n",
      "----\n",
      "iter 3000, loss: 66.421063\n",
      "----\n",
      " bamena\n",
      "inn, whe of mer ara daly lnleneedele youkone ff wuamingerek, neey was seme,\n",
      "re thwis in!.\" wos had ing.\n",
      "\n",
      "I sPos siveumed\"\n",
      "vy heensby masa bisuspedlysel t tf\"I heang ser aklt co thon, pinc. I fa \n",
      "----\n",
      "iter 3100, loss: 65.781424\n",
      "----\n",
      " sin weus,, Bfve rat ondore,, that of troud ay and fthlee Bust in arawl oved areb, sastidhind undleeg af ill the feat\n",
      "min penecere fhe\n",
      "sitre, herpaver r ith sik of biitsivind Atoqey king couslstef wot  \n",
      "----\n",
      "iter 3200, loss: 65.346327\n",
      "----\n",
      " peo tory wou; it thacn lang . and faviving moubind, . Bhis thiF sinzbe lowt, ot fey t meutain yow horte diln\n",
      "thed at bonth tingtshann\n",
      "atokandsthed js calsrtild to; to saathy andedn wit wiflvos pas, as \n",
      "----\n",
      "iter 3300, loss: 64.755421\n",
      "----\n",
      "  waenLe drevis akokitg cy afen an\";\n",
      "jag hith thindsqun hat peerthed neas, thalc. brap fonzs lit To the hone hinghed\n",
      "adly hot ho or wom the\n",
      "d ont coupe smoen guplias.T\n",
      "\"NAL\n",
      "POAAHHT8\"HHDL6(SMAY _THEITDE \n",
      "----\n",
      "iter 3400, loss: 64.252056\n",
      "----\n",
      " owd ilang be m te hes mut theed mee goblthim! noos comled incted sisgsigh, the wine imenins bat why oug ere honanps ale aw thid thee oraibleen, seslit; wis shala\n",
      " sawh therd\n",
      "avin.\n",
      "t anm\n",
      "on on tumin tu \n",
      "----\n",
      "iter 3500, loss: 63.832135\n",
      "----\n",
      " es therw weod cealeent, thist-sappasty, bo hirsly avouss, yhse ssst aus. For go cyrs sasreof thed tout mithing, worat wil koucurpadrasg oferytalgp, whow\n",
      "pill mollk as worst.\n",
      "'HEky\n",
      "ang -hat it whtworod \n",
      "----\n",
      "iter 3600, loss: 63.590905\n",
      "----\n",
      " he I fatct, loy tha resdapwor cil th onlfroelcokt lamy olef\n",
      "'n fall wheorpnd in aftincoutisbag\n",
      " heC firee. \n",
      "levens hadindess fapagwing ind won chat chy sithor ime is thowberh sh wy owte csitloon the w \n",
      "----\n",
      "iter 3700, loss: 63.396365\n",
      "----\n",
      " f sodeled watOse bulikeit peret and on todedor pohe ferby\n",
      "paclinga, pat sromews neltirt rares tanke.\n",
      "Band cheom -if sowef mortod, sut orecrerel rore.\n",
      "IBSI\n",
      "\n",
      "WBYuB\n",
      "\"SSHE KSCOOT TETAM\n",
      "-UBRVW,HOMESYII;, B \n",
      "----\n",
      "iter 3800, loss: 63.424619\n",
      "----\n",
      " watit too gill knite hlat he as\n",
      "he sidot sabily cane, th oug tho wap powikpr butn thy oue in wad the l an thl\n",
      "ceal. or culi-ss. chat bfard \n",
      "ing amlehis ais. Muy as ofghint wiut rut rorin doke tans thi \n",
      "----\n",
      "iter 3900, loss: 63.127998\n",
      "----\n",
      "  in, rimead t I-3te\n",
      "femper ibeit oftely rit liad thin ire, dife the at\n",
      "doant s. Suf oul y te has and ht-the on it og dea\n",
      "pat os wo the rporp as oe? bn, the-ing thitla'd s arh he chdzfad lengat of boun \n",
      "----\n",
      "iter 4000, loss: 62.709544\n",
      "----\n",
      " nind darkis buset bach anlile cadtiriss thin thin proteang. Wuses Fos cood thand lackus alppeng feind ofet and morlupathes; Fprool the Iresoesth and an\n",
      "tof\n",
      "spsif hasr chingt\n",
      "elarkigg buman. Mon tesh a \n",
      "----\n",
      "iter 4100, loss: 62.283897\n",
      "----\n",
      " rled lilk \n",
      "dlit more the the the lors mo sake ry. S\n",
      "mS\n",
      "ved.\n",
      "ing at, acly slia laok ut a dath s-sie illaint; nereer urtroll aD g wirly a gine mat serebig, eebled feaged, boof the thliped-\n",
      "\n",
      "  mes cusa a \n",
      "----\n",
      "iter 4200, loss: 62.197363\n",
      "----\n",
      " od ue apar, hith hint mow that ver. or to mut nuttom is to ssthonl\n",
      " Aos aot an a lhttto the dac;h a whenbole coald ves jemy af wifhs,e hand for pin, thy hes, peove thase thet or\n",
      "the te Sersun t\" Thill \n",
      "----\n",
      "iter 4300, loss: 61.999796\n",
      "----\n",
      " spoor, aksons moAs pstoked vere the s mf ant lofous\n",
      "sis he  lash\n",
      "saus Jonle colin yory, e it the t Wom racmhad holt aoj-fregapsmputor shat seape cancnd heell Mocctice thist stoist soxthe ame be daplas \n",
      "----\n",
      "iter 4400, loss: 61.871407\n",
      "----\n",
      " erowt, andy pangoud. The sthe gricint; nnathen wikimer're onais is. Agnonhurpinmived glot ing simqulivemys dle sange ceolltumst, thell the domble; irss. TL'I iud ale; alle toly. therss noamenleale, ar \n",
      "----\n",
      "iter 4500, loss: 61.606111\n",
      "----\n",
      "  acadme cromeprimy ong aplid of, the loantirt, Jteacetus a carpontsrls hee an!s eno gond lit to homy whang ibetad, Yois the the\n",
      " hes in' ly mot in. aly the. I it sytas manrer, wat foon at thirlact\n",
      "inl \n",
      "----\n",
      "iter 4600, loss: 61.162728\n",
      "----\n",
      " s to thi, Jouthim th h hey, onl erithint bem, Jveiwgh, an hikcsrls boterwheul nunastes anssewh sor hand ace sh low ror Afyt than erethe sent mremw\n",
      "Sbed the thii tovint yilestle haritar tlituld aerl I  \n",
      "----\n",
      "iter 4700, loss: 60.946103\n",
      "----\n",
      " ichon bur wiwesthe sor\n",
      "toem if shere any dceye\n",
      "ewhe. And how la g itht swini d, they ye\n",
      "woupheen the drire\n",
      "du\n",
      "shis, the platand hons, mman ouend hein syith thared\n",
      "to woIand cacshe pale iu hibow he sis \n",
      "----\n",
      "iter 4800, loss: 60.489345\n",
      "----\n",
      "  Gow bey, the hat one; tose hres Gren bon thasw mustowhonls stomias merkis this a theindhing weaiters decint is sig derl aOm. soogh bee\n",
      "bey dere hang. Heeche and hod cufmint Nof Jos bnceveuof, an! wim \n",
      "----\n",
      "iter 4900, loss: 60.110281\n",
      "----\n",
      " otan bit leanaokns daon aes the ig cad.a dersom akmuns and ber pellat\n",
      "thiket leated beas-ind fhy and ase in nore ua the lind the in loud lerer, ye st ar the propat frimangeme cerserlouCsem herisede an \n",
      "----\n",
      "iter 5000, loss: 59.911248\n",
      "----\n",
      " s.\n",
      "\n",
      "be that merent raers aven to sithn statare\n",
      " a moebe thisnot;\n",
      "seonpacke cikoure mupivant; whe to ma mound moron\n",
      "train-eros har, und ellascsen eunbllancat\n",
      "nof falsoliminn in te gacend bouf lroslaled \n",
      "----\n",
      "iter 5100, loss: 59.673186\n",
      "----\n",
      " r'd her wisshigg shrarime ly ruettons-brones cout hak mra coomd the al in in\n",
      "be althe leshat has twous\n",
      "themtrland stouma s, geres betta\n",
      "lingle. As aag saree. Yean ardasd, wiwe talle iptof buing te car \n",
      "----\n",
      "iter 5200, loss: 59.443205\n",
      "----\n",
      " d and aned whowlrusee. Hm and, iss ang onod the tabutosNishen thaty in ta\n",
      "fised, pod geert noule f\n",
      "cungarsich; ard wowes, Hallest faln and wo me theed? fure. Lo lill\n",
      "bins, cam serprelra ibow tas coud  \n",
      "----\n",
      "iter 5300, loss: 59.427994\n",
      "----\n",
      "  toll of by thin warpuyt all wrsthit wimrt. Whesrarond wiv the oot cowreitall ong qu ande faik; it un? The soot nos stuch indm mameree dmasthewusinfw\n",
      "sif, waeg and bars, initsev, Stiind a hin e onatro \n",
      "----\n",
      "iter 5400, loss: 59.217017\n",
      "----\n",
      " , des Lhen, jroo the and an ss wot slens to sen and ceorwutfed, it Csome Mxre, und neriplery. I awd paeshed doat tolhed at wuke sod fles.\n",
      "\n",
      "and, oadea a con and laokecup bf.\n",
      "\n",
      "\"AT1TAang, apl; sfeangede  \n",
      "----\n",
      "iter 5500, loss: 59.148915\n",
      "----\n",
      "  hasill muddas thedd lyle thig pod; Te\n",
      "of\n",
      "a god soul; an\n",
      " ountudhint. An in tha fay fime the purms waving sostuby an amurged do of Sh nom laterstef ons; ind to to, the serto, and wole, contor; asce Jo \n",
      "----\n",
      "iter 5600, loss: 58.892058\n",
      "----\n",
      " surist and whururn pimeng bed anded hemd ankorliand thot the borcerbttone urere oa his do, it ceacprmod ding ag her. bana\n",
      "ith onecie unes, whey wat rigobiit; the hew hip his horege trole wos wher, wha \n",
      "----\n",
      "iter 5700, loss: 58.755666\n",
      "----\n",
      " ve\n",
      "\n",
      "Bucneqerd me asing sout prm aned. mureeste gave goot ban to siwuat\n",
      "with in carhiwg loud\n",
      "st avig mased ogmommnigklewe\n",
      "corloll le sad carlbed ofond in he belped oule prige, and a-ses thoud in\n",
      " Jatpo \n",
      "----\n",
      "iter 5800, loss: 58.710749\n",
      "----\n",
      " nirg iteen mamer wag---- SubifGah\n",
      "A gevean? das\n",
      "thaksivebed the michike aghr,  arithed oel thite wor, mod aste magk hils und to\n",
      "\n",
      "fuedht the sor thiig, thonminh himelviin mir qovend ta parin ard oins;  \n",
      "----\n",
      "iter 5900, loss: 58.668203\n",
      "----\n",
      " rord to evaugoulk Qurlequeg hle hent as Opeane soens seer, higk arne in'te cafotqu beeld ung avonged-lipk howed anden thattuem. jarlinguttiringed ans the-urmes allieg, so l, sheld. Nas to ad and. iise \n",
      "----\n",
      "iter 6000, loss: 58.432027\n",
      "----\n",
      " escit, bang so ancus an dare un war the urtorny the, co geam,\n",
      "meen apet; Whard, gen caakeen madist\n",
      "mornest of cllith on woutputnp, sot icingmyy os, oom the mrols ytar an lrecudd\n",
      "s.\n",
      "Cwo hits aw--desed  \n",
      "----\n",
      "iter 6100, loss: 58.325038\n",
      "----\n",
      " t\n",
      "oneissme The yeale\n",
      "xuth a the sont an is heseats; sowlurreg has the Norans whoc otkaighter; sorliin--ancus the thd ofeat hise thek eackbe nadhinind end bo aund Nenlinl sesery icg ookat, andin? in th \n",
      "----\n",
      "iter 6200, loss: 58.289909\n",
      "----\n",
      " e eroring frilgirg the to I aluedint.\n",
      "\n",
      "nosshingsiund of omptlchiquet\n",
      "and I ffotmry Prat was roos car wldely ast watrores chat bouncu\n",
      "beg tuchos wigededed macou strowawiin and se kec\n",
      "the ver nour kige  \n",
      "----\n",
      "iter 6300, loss: 58.154680\n",
      "----\n",
      "  in he whiss to tA, Qureeg mo) ofestere pig wheu to se sersttari:csher. a'thaez ar, os ig s\"orry coresy\n",
      "bis herliighith libuut Porender ofy moly-ar it Heysey mrivctamoulk, ent this intes as oukes. Hor \n",
      "----\n",
      "iter 6400, loss: 58.212856\n",
      "----\n",
      " imd thoegos\n",
      "in his-aN jo, toon lingh more' bechid t llountteacny of, buckpellotnonleing and sowedch tirk, kerifostect of his coum is of caponey thas byelre?\n",
      "\n",
      "nat's ciksoensigk nod hein,\n",
      "\n",
      "hisung, in or \n",
      "----\n",
      "iter 6500, loss: 58.247404\n",
      "----\n",
      " ala,, hit thit at thect fike takeed nelly fewess\"rit phorsy teew; to ntoming whed waedeacver,\n",
      "date in tavend alre, thot andin't meenesh a\n",
      "thlo sulmingop bugher and hate had fow pefpatr as I ones?d tha \n",
      "----\n",
      "iter 6600, loss: 58.478200\n",
      "----\n",
      " bung rrind\n",
      "watithh cind veithe, all rody to\n",
      "thatind;--I the\n",
      " ard beres, leon, yering eomy an hike obaga micbighing ofcher'; thas the ponMhed solluog; wingqhit\n",
      "upyt, bot of porcufqded coon att merlit,  \n",
      "----\n",
      "iter 6700, loss: 58.384128\n",
      "----\n",
      " olk anw ows. the und ya can slou theure? Qutint wapcound h whey solasssout ousun ey.a\n",
      "\n",
      "Iaaste\n",
      "tos her of inituthisheded shem--\"hassfore manh anr-the to ta the yoes\n",
      "thor wrone\n",
      "thand dant ceatpe ficloge \n",
      "----\n",
      "iter 6800, loss: 58.261187\n",
      "----\n",
      " is? po the toruw. Bow inhshere\n",
      "apiceinondeen thind t I san fote; seas, fadon't\n",
      "yege eing beapee I the loms. I bes! aed'e.a\n",
      " optond hatand outhe Bobpsestouit lall towe whicht tand gavnf wos ucetwa, and \n",
      "----\n",
      "iter 6900, loss: 58.109870\n",
      "----\n",
      " nurer. ker dorcord a palinttal! feaogen sa theasee acd in ,iss\n",
      "then itise a the paverleordeen?-Pald the careees is thandus in loit the\n",
      "ouchint wo stowd\n",
      "phesentawamal, outangd deat; a t the\n",
      "cnaw, an.-\n",
      " \n",
      "----\n",
      "iter 7000, loss: 57.976182\n",
      "----\n",
      " ose ileditaadmid\n",
      "cealalicilit a nowr, hara\n",
      "eseinsoucatathand loe thase and lad, caid b lrrostasobne a ftook;s, thenes hinticheculoherighy Hlenat in'! lim tacitt. Som tHeir lonebinn bocoua, a in baderl \n",
      "----\n",
      "iter 7100, loss: 58.028527\n",
      "----\n",
      "  nit ay ond, astaded-tickiild\n",
      "are.\n",
      "\n",
      "No lis, and tut octartonging\n",
      "Ceapofsod aryis thit, inct soa\n",
      "caale on outon to the nis ba hian..\n",
      "B Wullr coeling,\"\n",
      "\n",
      "Non do. The; diss, to in ACwrelbor,\n",
      "Po cape seing \n",
      "----\n",
      "iter 7200, loss: 58.075106\n",
      "----\n",
      " os pellt-,\n",
      "ire lam't beat le und, vememsHy hivent med uve, was orod thI laps oWlat\n",
      "Pofthe in neplred'n sat gop, sholgh miI and wain that use bofoeg, thit oullo prof a fnop I andind iby benged.\n",
      "I't con \n",
      "----\n",
      "iter 7300, loss: 57.823839\n",
      "----\n",
      "  lere Ploigey--ralain sat un te her thoncang tadBulning in Af noterting kion hoav vine the ied\" shald ha mrattsurisss ther the an\n",
      "far', hes mug wap nos in in ta tasld of wof the fruls, ut pcoul thacy  \n",
      "----\n",
      "iter 7400, loss: 57.842686\n",
      "----\n",
      " ark ald sovanPeeqontted save thon's, Bued, furend ind the Cound-s opfis nothhy gha eny hoonting yo bured ampere-sive\n",
      "Boncers he bowliduthing, Bexs\n",
      "than Post har in to maly, llle. Cometitcring and bo h \n",
      "----\n",
      "iter 7500, loss: 57.783164\n",
      "----\n",
      "  thes mate mact namr uever. I's' edutd apver, Aor a dand besule cand iphe he pil co-sarenjimy fooble say aw\n",
      "onp mack whevent incene ied withe leaver cith gim enas, alens thit seinqu, plathar cald loon \n",
      "----\n",
      "iter 7600, loss: 57.791009\n",
      "----\n",
      " d the mes,, ik me thasere I pad, yutobrat, sort he I ontharcucn; Anfase sopenoun whou-gred aitt, wir\n",
      "wuiteld bout honernon of\n",
      "in of ofsthed, binsest! rorele wart nobeo daold thit caid. I PAR fercoted  \n",
      "----"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('moby_dick.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Heavy Lifting - LSTM\n",
    "Discussion of Long Short Term Memory from [Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from the above blog:\n",
    "- Cell state\n",
    "- Logic gates with learned parameters to update state (All sigmoids)\n",
    "- Normal Activation is tanh to cell state and output of tne node\n",
    "- Forget Gate\n",
    "- Input Gate\n",
    "- Output Gate\n",
    "\n",
    "Not discussed here:\n",
    "- How do we update the weights of the gates?  Same as always, just change the math based on the activation function it when through.\n",
    "- Vannishing error gradients - prevented by Error Carusel\n",
    "- Error Carousel (propogate error, until network learns the proper layers to diminish it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
